<!DOCTYPE html>
<html lang="ko">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>[대신증권 서영재] 커스텀 AI 칩, 브로드컴과 마벨</title>
    <style>
        body {
            font-family: Arial, sans-serif;
            line-height: 1.6;
            color: #333;
            margin: 20px;
        }

        h1 {
            font-size: 2em;
            color: #0056b3;
            margin-bottom: 20px;
            text-align: center;
        }

        h2 {
            font-size: 1.5em;
            color: #0056b3;
            margin-top: 30px;
            margin-bottom: 10px;
        }

        p {
            font-size: 1.1em;
            margin-bottom: 15px;
            text-align: justify;
        }

        table {
            width: 100%;
            border-collapse: collapse;
            margin-top: 20px;
            margin-bottom: 30px;
        }

        th, td {
            border: 1px solid #ddd;
            padding: 8px;
            text-align: left;
        }

        th {
            background-color: #f2f2f2;
            font-weight: bold;
        }

        .executive-summary {
            font-style: italic;
            color: #555;
        }

        strong {
            color: #007bff;
        }
    </style>
</head>
<body>

    <h1>[대신증권 서영재] 커스텀 AI 칩, 브로드컴과 마벨</h1>

    <p class="executive-summary">
        커스텀 AI 칩 비중을 늘리기 시작한 클라우드 기업들<br>
        AI 사이클 초기에는 클라우드 기업들이 AI 경쟁에서 뒤처지지 않기 위해 공격적으로 CAPEX를 확대했다 . 그러나 ChatGPT를 필두로 시작된 AI 사이클이 곧 4년차로 접어들면서 CAPEX 기조가 미묘하게 변하고 있다. 첫째로 , 클라우드 기업들의 CAPEX 증가율이 고점을 찍고 하락하고 있다. CAPEX 증가율은 2024년 +54% YoY → 2025년 +51% YoY → 2026년 +17% YoY 수준으로 둔화될 것으로 예상되고 있다. AI 수요는 여전히 가파르게 증가하고 있으나 , 칩 공급여력이 확대되면서 쇼티지 환경이 완화되고 있기 때문이다 . 둘째로 , 클라우드 기업들의 CAPEX 비중이 변하고 있다. 사이클 초기에는 클라우드 기업들의 CAPEX 투자 대부분이 엔비디아 범용 AI 칩 중심으로 이루어졌지만 , 최근 들어 커스텀 AI 칩 비중이 늘어나고 있다. 이에 따라, 엔비디아 데이터센터 매출 대비 커스텀 AI 칩 & 네트워킹 시장 규모 비율은 2024년 18.3%에서 2027년 34.4%까지 상승할 전망이다 . 커스텀 AI 칩 시장은 향후 3년 동안 최소 3배 이상 커질 전망<br>
        변화의 신호탄을 쏜 것은 브로드컴의 커스텀 AI 칩 시장 전망 발표다. 브로드컴은 FY 4Q24 실적 발표에서 2027년 커스텀 AI 칩 & 네트워킹의 서비스 가능 시장 규모를 600~900 억 달러로 전망했다 . 서비스 가능 시장은 브로드컴이 커스텀 AI 칩 개발 계약을 맺고 있는 기업 중 알파벳, 메타 플랫폼스, 바이트댄스를 기반으로 산출한 규모로, OpenAI와의 100억 달러 규모 계약은 포함되지 않은 수치다. OpenAI 외 3개 기업과도 계약을 논의 중인 점을 감안하면 서비스 가능 시장 규모는 상향 조정될 것으로 예상한다 . 2024년 커스텀 AI 칩 & 네트워킹 전체 시장 규모(TAM, Total Addressable Market) 가 150~200 억 달러로 추정되는 점을 감안하면 향후 3년 동안 시장이 최소 3배 이상 커질 것으로 전망한다.<br>
        커스텀 AI 칩은 1) AI 칩 설계 격차 축소, 2) 네트워킹 기술 역량을 기반으로 TCO 측면의 우위를 공략하고 있다.<br>
        주요 기업: 브로드컴 (AVGO), 마벨 (MRVL) 최선호 기업으로 브로드컴을 제시한다 . 브로드컴의 커스텀 AI 칩 시장 점유율은 2024년 기준으로 최소 60% 이상으로 추정된다 . 고객사와 다년간에 걸쳐 여러 세대의 커스텀 AI 칩을 설계하는만큼 점유율은 안정적으로 유지될 것으로 판단한다. 주요 고객인 알파벳, 메타 플랫폼스 , 바이트댄스 , OpenAI가 데이터센터 투자를 공격적으로 늘리는 점도 커스텀 AI 칩 수요를 견인할 전망이다 . TCO를 낮추는데 중요한 커스텀 AI 칩 설계 & 이더넷 네트워킹 모두 강점을 가지고 있는만큼 시장 성장에 따른 수혜를 입을 것으로 판단한다 . 관심 기업으로 마벨을 제시한다 . 마벨은 아마존, 마이크로소프트를 주요 고객으로 확보하고 있다. 다만, 아마존은 클라우드 기업 내 CAPEX 증가폭이 상대적으로 떨어지고 , 마이크로소프트의 커스텀 AI 칩 개발 계획은 연기되었기에 주가가 모멘텀을 받기 위해서는 시간이 필요해 보인다.
    </p>

    <h2>Part 1. 왜 커스텀 AI 칩인가</h2>

    <p>
    </p>

    <h2>I. 변곡점에 진입하는 클라우드 기업들의 CAPEX</h2>

    <h3>AI 사이클 시작 4년차, 미묘하게 변하기 시작한 CAPEX 기조</h3>

    <p>
        ChatGPT 를 필두로 시작된 AI 사이클이 곧 4년차로 접어든다 . 사이클 초입에는 클라우드 기업들 이 AI 경쟁에서 뒤처지지 않기 위해 공격적으로 CAPEX 를 확대했다 . 마이크로소프트 , 알파벳을 중심으로 한 클라우드 기업들의 CAPEX 규모는 2023년 1,560 억 달러 수준에서 2025년 3,658억 달러로 2년 만에 2배 이상 증가할 것으로 전망된다 . 동기간 엔비디아의 실적은 폭발적으로 증가했다 .<br><br>

        다만, AI 사이클을 주도하는 기업들의 CAPEX 기조가 미묘하게 변하고 있다. 첫째로 , 클라우드 기업들의 CAPEX 증가율이 고점을 찍고 하락하고 있다. CAPEX 증가율은 2024년 +55% YoY → 2025년 +51% YoY → 2026년 +17% YoY 수준으로 둔화될 것으로 예상되고 있다. AI 수요는 여전히 가파르게 증가하고 있으나 , 칩 공급여력이 확대되면서 쇼티지 환경이 완화되고 있기 때문이다 . AI 칩 성능이 향상되면서 동일한 CAPEX 규모에도 더 많은 컴퓨팅 캐파를 가지게 된 것도 영향을 미치고 있다.
    </p>

    <h3>CAPEX 증가율 둔화, CAPEX 내 비중 변화</h3>

    <p>
        둘째로, 클라우드 기업들의 CAPEX 비중이 변하고 있다. 사이클 초기에는 클라우드 기업들의 CAPEX 투자 대부분이 엔비디아 범용 AI 칩 중심으로 이루어 졌지만 , 최근 들어 커스텀 AI 칩 도입을 늘리고 있다. AI 사이클 초기 엔비디아의 유력한 경쟁자로 거론되던 기업은 AMD, 인텔이었다 .<br><br>

        엔비디아 대비 성능은 조금 떨어지더라도 , 가격을 크게 낮춰 TCO(Total Cost of Ownership, 총 소유비용 ) 측면에서 우위를 부각시킬 수 있다는 가능성 때문이었다 . 그러나 엔비디아가 가지고 있는 범용 AI 칩 설계, 소프트웨어 , 네트워킹의 강점이 견고하다보니 , 후발주자인 AMD 와 인텔이 3년이라는 시간에 격차를 빠르게 좁히기는 역부족이었다 (24/03/11 AI 가속기 , 추론의 시대). 오히려 , 커스텀 AI 칩을 설계하는 브로드컴 , 마벨이 엔비디아의 경쟁자로 떠오르고 있다.<br><br>

        커스텀 AI 칩 기업들은 AMD, 인텔이 보여주려 했던 TCO 측면에서의 우위를 바탕으로 클라우드 기업들 의 CAPEX 를 공략하고 있다. 커스텀 AI 칩 가격은 엔비디아 범용 AI 칩의 절반 이하로 추정된다 . 아울러 , 브로드컴은 네트워킹 시장 1위 역량을 바탕으로 수만, 수십만 개의 AI 칩이 빠르고 , 정확하게 통신할 수 있는 환경을 구축할 수 있다. 개별 커스텀 AI 칩의 성능은 범용 AI 칩 대비 떨어지더라도 , 기업 수요에 맞게 특화 설계해 효율성을 높이고 , 네트워킹 역량을 바탕으로 AI 칩의 연결성을 확대하고 있다. 이를 통해, 일부 워크로드 의 경우 TCO 측면에서 범용 AI 칩 대비 우위를 보이는 것으로 추정한 다. 일례로 , 아마존은 AWS에서 훈련용 커스텀 AI 칩 Trainium 2 를 통해 EC2 인스턴스를 처리할 경우 GPU 대비 30%~40% 저렴하다고 밝힌 바 있다.
    </p>

    <h2>II. 변화의 신호탄을 쏜 브로드컴의 커스텀 AI 칩 시장 전망</h2>

    <h3>커스텀 AI 칩 시장의 가파른 성장을 전망</h3>

    <p>
        변화의 신호탄은 브로드컴의 커스텀 AI 칩 시장 전망 발표였다 . 브로드컴은 FY 4Q24 실적 발표에서 2027년 커스텀 AI 칩 & 네트워킹의 서비스 가능 시장 규모(SAM, Serviceable Available Market) 를 600~900 억 달러로 전망했다 . 서비스 가능 시장은 브로드컴이 커스텀 AI 칩 개발 계약을 맺고 있는 기업 중 알파벳 , 메타 플랫폼스 , 바이트댄스 를 기반으로 산출한 규모다 .<br><br>

        브로드컴은 이들 기업 외에도 양산 일정이 구체화되지 않은 4개 기업과도 커스텀 AI 칩계약을 맺은 상황이었는데 , 이번 실적 발표에서 그 중 하나인 OpenAI 와 100억 달러 규모의 양산 계약을 체결했음을 밝혔다 . OpenAI 의 계약을 단순 합산할 경우 2027년 커스텀 AI 칩 & 네트워킹 시장 규모는 700~1,000 억 달러로 증가한다 . 아직 양산 계획이 구체화되지 않은 기업들까지 추가할 경우 서비스 가능 시장 규모는 더욱 커질 전망이다 .<br><br>

        브로드컴 이 2024년 커스텀 AI 칩 & 네트워킹 전체 시장 규모(TAM, Total Addressable Market) 를 150~200 억 달러로 추정한 점을 감안하면 향후 3년 동안 시장이 최소 3배 이상 커질 것으로 전망된다. 브로드컴의 FY 2024 커스텀 AI 칩 & 네트워킹 매출이 122억 달러인 점을 고려하면 , 시장 점유율은 최소 60% 이상으로 추정된다 . 클라우드 기업들의 CAPEX 증가율 이 2026년부터 둔화되는 점을 고려하면 , 향후 CAPEX 내 커스텀 AI 칩 비중은 점진적으로 상승 추세를 보일 전망이다 .
    </p>

    <h3>엔비디아 데이터센터 매출 대비 커스텀 AI & 네트워킹 시장 규모 비율은 2027년 34.4%까지 상승 전망</h3>

    <p>
        엔비디아 데이터센터 매출 대비 커스텀 AI 칩 & 네트워킹 시장 규모 비율은 2024년 18.3% 에서 2027년 34.4%까지 상승할 전망이다 . 2024년~2027 년 엔비디아 데이터센터 매출은 1,090 억 달러에서 2027년 2,900억 달러로 증가할 전망이다 .<br><br>

        그러나 커스텀 AI 칩 & 네트워킹 시장 규모는 동기간 150~200 억 달러에서 700~1,000억 달러로 더 빠르게 커질 것으로 예상된다 . 브로드컴이 계약을 맺은 기업 외 마벨을 비롯한 여타 커스텀 AI 칩 기업까지 포함하면 비율은 더 높아진다.
    </p>

    <h3>반도체/하드웨어에서 소프트웨어로 이동하는 AI 트렌드</h3>

    <p>
        브로드컴 의 대표 커스텀 AI 칩 고객은 알파벳 과 메타 플랫폼스다 . 브로드컴은 2014년부터 알파벳과 10개의 커스텀 AI 칩을 공동 개발하고 있다. 알파벳이 TPU(Tensor Processing Unit) 로 불리는 커스텀 AI 칩을 설계를 시작한 이유는 2010년대 중반부터 늘어난 음성인식 , 이미지 처리 등 딥러닝 서비스에 대응하기 위함이다. GPU 대비 TCO 관점에서 우위에 있는 칩을 설계하는 것을 목표로 한다. 2025년 4월 공개된 7세대 TPU, Ironwood 는 대규모 추론에 특화된 칩이다 . 6세대 TPU 대비 컴퓨팅 파워가 10배 향상된 가운데 , 전력 효율은 2배 증가했다 . 최대 연산 성능은 FP8 기준 4,614 TFLOPS (Tera Floating Point Operations Per Second , 1초당 1조회 부동소수점 연산) 수준으로 엔비디아의 B200 칩과 유사한 수준이다 .<br><br>

        메타 플랫폼스도 브로드컴과 지난 4년간 4개의 커스텀 AI 칩을 공동 개발했다 . 2025년부터 MTIA 2(Meta Training & Inference Accelerator 2) 본격적으로 양산하면서 브로드컴 매출에 본격적으로 기여하기 시작했다 .
    </p>

    <table>
        <thead>
            <tr>
                <th>세대</th>
                <th>출시연도</th>
                <th>공정</th>
                <th>칩당 HBM 용량</th>
                <th>칩당 메모리 대역폭 (GB/s)</th>
                <th>전력 소모</th>
                <th>최대 연산 성능 (TOPS, TFLOPS)</th>
            </tr>
        </thead>
        <tbody>
            <tr>
                <td>TPU v1</td>
                <td>2015</td>
                <td>28nm</td>
                <td>8 GB DDR3</td>
                <td>34</td>
                <td>40W</td>
                <td>92 TOPS (8 -bit)</td>
            </tr>
            <tr>
                <td>TPU v2</td>
                <td>2017</td>
                <td>16nm</td>
                <td>16 GB HBM</td>
                <td>600</td>
                <td>280W</td>
                <td>45 TFLOPS (bfloat16)</td>
            </tr>
            <tr>
                <td>TPU v3</td>
                <td>2018</td>
                <td>16nm</td>
                <td>32 GB HBM</td>
                <td>900</td>
                <td>220W</td>
                <td>123 TFLOPS (bfloat16)</td>
            </tr>
            <tr>
                <td>TPU v4</td>
                <td>2021</td>
                <td>7nm</td>
                <td>32 GB HBM</td>
                <td>1,200</td>
                <td>170W</td>
                <td>275 TFLOPS (bfloat16)</td>
            </tr>
            <tr>
                <td>TPU v5e</td>
                <td>2023</td>
                <td>미공개</td>
                <td>16 GB HBM</td>
                <td>819</td>
                <td>200W</td>
                <td>197 TFLOPS (bfloat16)</td>
            </tr>
            <tr>
                <td>TPU v5p</td>
                <td>2023</td>
                <td>미공개</td>
                <td>95 GB HBM</td>
                <td>2,765</td>
                <td>미공개</td>
                <td>459 TFLOPS (bfloat16)</td>
            </tr>
            <tr>
                <td>TPU v6e</td>
                <td>2024</td>
                <td>미공개</td>
                <td>32 GB HBM</td>
                <td>1,640</td>
                <td>미공개</td>
                <td>918 TFLOPS (bfloat16)</td>
            </tr>
            <tr>
                <td>TPU v7</td>
                <td>2025</td>
                <td>미공개</td>
                <td>192 GB HBM</td>
                <td>7,200</td>
                <td>미공개</td>
                <td>4,614 TFLOPS (fp8)</td>
            </tr>
        </tbody>
    </table>

    <h3>반도체/하드웨어에서 소프트웨어로 이동하는 AI 트렌드</h3>

    <p>
        알파벳 , 메타 플랫폼스 , 아마존은 향후 커스텀 AI 칩을 늘릴 계획임을 명확하게 밝히고 있다. 알파벳은 2024년 4분기 실적 발표에서 향후 데이터센터에 탑재하는 AI 가속기를 대부분 커스텀 AI 칩으로 채울 계획이라고 발표했다 .<br><br>

        “We also look at every investment that we make to ensure that we're doing it in the most cost-efficient way to optimize our data center. Our strategy is mostly to rely our own se lf design and build data centers. So they're industry -leading in terms of both cost and pow er efficiency at scale ”<br><br>

        메타 플랫폼스 도 2024년 4분기 실적 발표에서 커스텀 AI 칩 도입 확대 계획을 발표했다. MTIA 칩을 AI 콘텐츠 추천 모델 추론에 우선 적용한 뒤, 내년부터는 AI 콘텐츠 추천 모델 훈련에도 사용할 계획이다 . 이는 커스텀 AI 칩이 훈련과 추론 전반으로 확장하고 있음을 나타낸다 .<br><br>

        “We're pursuing cost efficiencies by deploying our custom MTIA silicon in areas where we can achieve a lower cost of compute by optimizing the chip to our unique workloads. In 2024, we started deploying MTIA to our ranking and recommendation influence worklo ads for ads and organic content. We expect to further ramp adoption of MTIA for these use cases throughout 2025, before extending our custom silicon efforts to training workloads for ranking and recommendations next year.<br><br>

        아마존도 2024년 4분기 실적 발표에서 앤트로픽과 Trainium2 수십 만개를 탑재한 Project Rainier 를 진행하고 있다고 밝혔다 . 이는 컴퓨팅 파워 기준으로 앤트로픽이 기존에 사용하 던 데이터센터보다 5배 더 큰 규모다.<br><br>

        “We're already hard at work on Trainium3, which we expect to preview late in 2025 and defining Trainium4 thereafter. Building outstanding performant chips that deliver leading price performance has become a core strength of AWS ”<br><br>

        클라우드 기업들의 커스텀 AI 칩 관련 코멘트들은 브로드컴의 공격적인 커스텀 AI 칩 시장 성장 전망을 뒷받침한다 .
    </p>

    <h3>Consumer AI</h3>

    <p>
        브로드컴이 커스텀 AI 칩을 공동 개발하고 있는 알파벳 , 메타, 바이트댄스 는 Consumer AI 분야에 속해 있다. Consumer AI 는 참여 기업 수는 매우 적지만 , 개별 기업별로 수십억 명의 유저를 서비스하는 구조다 .<br><br>

        Consumer AI 기업들은 유저들이 플랫폼을 오래 이용해 광고를 많이 볼수록 , 광고 전환율이 높아질수록 매출이 증가한다. 최근 Consumer AI 분야에서 관찰되는 특징은 AI & 머신러닝에 투자하는 금액에 비례해 유저의 플랫폼 참여가 상승한다는 것이다 . 대표적으로 메타 플랫폼스는 유저들에게 콘텐츠를 추천해주는 AI 모델, 광고를 매칭시켜주는 AI 모델 개발에 꾸준히 투자하고 있다. 이를 통해 유저의 이용시간과 광고 전환율을 높여 매출을 높이고 있다(25/05/01 메타 플랫폼스 , AI 추천 모델 & Direct Response가 상쇄하는 광고 업황 둔화).<br><br>

        메타 플랫폼스 의 경우 2022년 하반기 부터 2023년 하반기 까지는 릴스 도입 & AI 콘텐츠 추천 모델 성능 향상에 따른 광고 노출 횟수 증가, 2024 년 상반기부 터 지금까지는 AI 광고 추천 모델 향상에 따른 광고 단가 상승이 매출 증대를 견인하고 있다. 2025년 1분기 기준으로 메타 플랫폼 이용자수가 34.3억 명에 달하는 점을 고려하면 개인별 AI 콘텐츠 추천 모델, AI 광고 추천 모델을 구동하기 위해서는 막대한 컴퓨팅 캐파가 필요하다 .<br><br>

        아래는 브로드컴 반도체 솔루션 부문 대표 찰리 카와스 의 코멘트다 .<br><br>

        “AI 가속기를 개발하는 방식은 두 가지가 있습니다 . 첫째는 다양한 기업의 니즈를 충족할 수 있는 범용 제품을 개발하는 것입니다 . 그러나 사용자 수가 수십억 명에 이르고 매출이 수천억 달러에 달하는 초대형 플랫폼을 운영하는 Consumer AI 기업의 경우, 범용 제품을 사용할 때 전력 소모가 지나치게 많고 비용도 큽니다 . 커스텀 AI 칩을 설계하는 것이 더 효율적 이라는 뜻입니 다. 커스텀 AI 칩들이 서로 협력하도록 할 때 중요한 것이 네트워킹입니다 . 앞으로도 네트워킹 기술은 범용 제품이 표준이 될 것입니다 ”<br><br>

        “기업별로 매출을 창출하는 데 있어 중요한 내부 워크로드 가 있습니다 . 따라서 기업 필요에 따라 AI 가속기 아키텍처 , 대역폭 , 입출력 비율을 커스터마이징하면 , 범용 하드웨어를 사용할 때보다 특정 워크로드를 훨씬 더 효율적으로 수행할 수 있습니다 ”
    </p>

    <h2>III. 커스텀 AI 칩이 범용 AI 칩 대비 가지는 장점</h2>

    <h3>커스텀 AI 칩이 TCO 측면 우위를 갖는 이유 1) 범용 AI 칩 설계 우위 축소</h3>

    <p>
        커스텀 AI 칩이 TCO 측면에서 범용 AI 칩과 경쟁할 수 있는 배경에는 1) AI 칩 설계 격차 축소, 2) 네트워킹 기술 역량에 기인한다 . 첫째로 , 엔비디 아의 AI 칩 설계 우위는 거센 추격을 받고 있다. 엔비디아가 신규 제품을 발표할 때마다 단일 칩의 성능은 향상되고 있다. 그러나 공정이 미세화될수록 단일 GPU를 설계하는 역량이 기여하는 부분보다 , 여러 GPU 다이를 연결하는 네트워킹 및 시스템 레벨 최적화가 미치는 영향이 더 커지고 있다.<br><br>

        단일 GPU의 컴퓨팅 역량은 mm2당 트랜지스터 수가 결정한다 . 과거 Pascal Volta, Ampere, Hopper 아키텍처를 거치는 동안 mm2당 트랜지스터 수가 꾸준히 증가하면서 성능 향상을 견인했다 . 그러나 엔비디아의 동일 면적당 트랜지스터 직접도 증가율은 V100 → A100 구간 +153.5%, A100 → H100 구간 +49.8%, H100 → B200 구간 +32.3% 로 점진적으로 낮아지는 추세다 . 과거에 레거시 공정을 통해 칩을 생산할 때는 기업들이 독자적인 방식으로 트랜지스터를 배치하고 , 구조를 바꾸는 것을 통해 칩의 성능을 크게 높일 수 있었다 . 그러나 5nm 이하 초미세 공정부터는 트랜지스터 크기가 작아지면서 , 전자가 예상치 못한 경로로 흐르거나 , 신호 간섭 같은 문제가 생기기 시작했다 .<br><br>

        이때부터는 기업들이 고유 설계 방법으로 문제를 해결하기보다는 칩 제조회사의 표준화된 공정과 설계 가이드라인을 따라 문제를 해결하는 경향이 생겼다. 이에 따라, 창의적인 설계를 통해 얻을 수 있는 성능 차이가 점차 줄어들고 있는 상황이다 . 이에 따라, 점차 설계 우위보다 칩의 연결, 시스템 레벨 최적화가 더 중요한 기술적 우위가 되고 있다.<br><br>

        한편, AMD 가 H100 의 경쟁 제품으로 발표했던 MI300X 의 경우 mm2당 트랜지스터 수가 H100 대비 2배 수준이었다 . 이는 AMD 가 네트워킹이나 소프트웨어 측면의 열위를 극복하기 위해 직접도를 최대한 높이는 방식을 선택한 결과로 보인다 . 다만, 이는 트랜지스터 간의 간섭이나 , 생산 과정에서의 수율 감소로 이어졌을 가능성이 높다.<br><br>

        단일 GPU를 설계하는 역량 차이가 점차 축소됨에 따라, ASIC 기업들이 일부 워크로드에서는 TCO 측면에서 우위를 점할 수 있는 환경이 마련되고 있다.
    </p>

    <table>
        <thead>
            <tr>
                <th>구분</th>
                <th>트랜지스터 수</th>
                <th>GPU 다이 사이즈 (mm2)</th>
                <th>GPU 다이 수 (개)</th>
                <th>mm2당 트랜지스터 수 (십억)</th>
                <th>직접도 증가율 (%)</th>
                <th>공정 (nm)</th>
            </tr>
        </thead>
        <tbody>
            <tr>
                <td>K40</td>
                <td>7</td>
                <td>551</td>
                <td>1</td>
                <td>0.13</td>
                <td>-</td>
                <td>28</td>
            </tr>
            <tr>
                <td>M40</td>
                <td>8</td>
                <td>601</td>
                <td>1</td>
                <td>0.13</td>
                <td>3.3</td>
                <td>28</td>
            </tr>
            <tr>
                <td>P100</td>
                <td>15</td>
                <td>610</td>
                <td>1</td>
                <td>0.25</td>
                <td>88.4</td>
                <td>16</td>
            </tr>
            <tr>
                <td>V100</td>
                <td>21</td>
                <td>815</td>
                <td>1</td>
                <td>0.26</td>
                <td>3.2</td>
                <td>12</td>
            </tr>
            <tr>
                <td>A100</td>
                <td>54</td>
                <td>826</td>
                <td>1</td>
                <td>0.66</td>
                <td>153.5</td>
                <td>7</td>
            </tr>
            <tr>
                <td>H100</td>
                <td>80</td>
                <td>814</td>
                <td>1</td>
                <td>0.98</td>
                <td>49.8</td>
                <td>4N</td>
            </tr>
            <tr>
                <td>B200</td>
                <td>208</td>
                <td>800</td>
                <td>2</td>
                <td>1.30</td>
                <td>32.3</td>
                <td>4NP</td>
            </tr>
        </tbody>
    </table>

    <h3>커스텀 AI 칩이 TCO 측면 우위를 갖는 이유 2) 네트워킹 역량</h3>

    <p>
        둘째로 , 네트워킹 역량이다 . 메타 플랫폼스는 2022년 열린 OCP(Open Compute Project) Global Summit 에서 AI를 사용할 때 네트워킹 의 중요성을 피력했다 . “Network I/O is key for Recommendation Workloads". 메타 플랫폼스가 밝힌 바에 따르면 주로 사용하는 4개의 머신러닝 모델 기반의 워크로드를 처리할 때 네트워킹에서 소요되는 시간이 18%~57% 에 달했다.<br><br>

        데이터가 네트워크에 머무르는 동안에 는 GPU가 가동되 지 않아 효율성이 크게 떨어진다. 개당 3만 달러의 GPU로 10만 개의 클러스터를 구축할 경우 데이터센터 하나에 최소 30억 달러라는 막대한 비용이 필요한 점을 고려하면 , 네트워킹에 병목 현상이 커질수록 경제적으로 큰 손해를 보게 된다.<br><br>

        네트워킹에서 병목 현상이 발생하는 이유는 크게 3가지다 . 1) 최고 사양의 CPU의 경우에 요구되는 네트워킹 대역폭은 50Gbps 수준인 반면, 최신 GPU의 경우 CPU의 최대 20배에 이르는 네트워킹 대역폭이 요구된다 . 2024년 최신 GPU의 경우 400Gbps 수준이었으나 2025년에는 800Gbps 네트워킹 대역폭이 사용되고 있다. GPU가 네트워크 자원을 매우 많이 소모하는 것이다 .<br><br>

        2) GPU 기반 작업이 가지는 특징 중 하나는 모든 GPU가 동시에 통신하는 경향을 보인다는 것이다 . 이는 직장인들이 아침 9시까지 출근하기 위해 동일한 시간에 나오면서 도로가 막히는 것과 유사하다 . 단, GPU의 경우에는 직장인들이 소형차가 아닌 대형 화물 트럭을 몰고 도로로 나오는 상황이라는 차이점이 있다.<br><br>

        3) GPU들은 다수의 네트워킹 라인이 있음에도 불구하고 , 하나로 몰리는 경우가 발생한다. 앞선 예시처럼 , 대형 화물 트럭들이 8차선 도로에서 1차선에만 몰리는 것이랑 동일하다. 네트워킹 라인을 효율적으로 사용하지 못하는 케이스들이 발생하는 것이다 . 또한, 일부 워크로드는 긴 시간 지속되는 경우가 있는데 , 이는 직장인들이 매일 긴 거리를 출근하는 것과 비슷하다 . 중간에 문제가 생기면 다시 출발 지점으로 돌아가 작업을 재시작해야 한다.<br><br>

        이러한 특성들은 네트워크 병목이 효율성을 낮추는 주된 요인들이다 .
    </p>

    <h3>브로드컴의 네트워크 역량 우위</h3>

    <p>
        브로드컴은 네트워킹 성능/가격 관점에서 엔비디아 대비 우위에 있는 것으로 추정된다 . 네트워킹은 랙 내부 칩 간 연결을 지원하는 Scale-Up, 랙 간 연결을 지원하는 Scale-Out으로 나뉜다 . Scale-Up의 경우 엔비디아는 NVLink, 브로드컴은 Tomahawk 제품으로 대응하고 있다. 엔비디아 NVLink 의 경우 72개의 GPU를 연결할 수 있고, 28.8 TB/s 대역폭을 가지는 반면, 브로드컴의 Tomah awk의 경우 512개의 XPU를 연결할 수 있고, 102.8 TB/s 대역폭을 지원한다 . 브로드컴 이 더 많은 칩을 더 넓은 대역폭으로 연결할 수 있다. Scale-Out의 경우 엔비디아는 Infiniband, 브로드컴은 Jericho 제품으로 대응하고 있다. 브로드컴이 지난 8월 출시한 Jericho 4 의 경우 51.2 TB/s 대역폭 을 지원하며 , 여러 데이터센터에 나뉘어져 있는 20만 개 이상의 칩을 연결할 수 있다. 궁극적으로는 100만 개 이상의 칩을 연결하는 것을 목표로 한다.<br><br>

        클라우드 기업들은 전력 부족, 부지 확보 등의 이유로 아직까지는 단일 데이터센터에 10만 개 이상의 칩을 탑재하는 것을 지양하고 있다. 대신, 100km 이상 떨어진 거리에 여러 개의 데이터센터를 나누어 짓는 방식을 취하고 있다. 이에, 데이터센터 간 네트워킹을 지원하는 것이 중요한 역량 중 하나로 떠오르고 있는데 , Jericho 가 이를 지원한다
    </p>

    <h3>이더넷은 Infiniband 대비 가격과 성능 측면에서 우위</h3>

    <p>
        Infiniband 는 고속, 저전력이라는 측면에서 강점을 가지나 , 초기 비용이 비싸고 , 오류가 발생할 경우 대처하는데 시간이 길게 소요된다는 단점이 있다. 이에, 가격 및 실용적인 측면에서 브로드컴의 이더넷이 상대적으로 선호되고 있다.<br><br>

        엔비디아는 2019년 멜라녹스를 인수하면서 네트워킹 역량을 강화했다 . 엔비디아는 멜라녹스의 Infiniband 네트워킹 기술을 적용하고 있으며 , 고성능 제품군에 주로 Silion Photonics( 실리콘 포토닉스 ) 기술을 적극적으로 활용하고 있다. 실리콘 포토닉스 는 실리콘 기반의 반도체 칩에서 기존에 활용하던 전자 기반 신호가 아닌, 빛(광자, Photonics) 를 이용하여 데이터를 전송하는 기술을 의미한다 . 전자 기반 방식보다 빠르고 , 저전력으로 데이터를 전송할 수 있다는 점에서 주목받고 있다. 아울러 , CPO = Co -Packaged Optics 방식을 활용하 여 실리콘 포토닉스 기반의 광통신 모듈을 반도체 칩 패키지 내부에 통합했다 . 이를 통해, 기존에 별로 분리되어 있던 광모듈과 네트워크 칩을 하나의 패키지로 통합하여 , 성능과 전력 효율을 크게 향상시켰다 .<br><br>

        엔비디아가 Infiniband 를 통해 AI 반도체와의 네트워킹 통합성을 높여갈 때만하더라도 , 브로드컴 의 이더넷 기반 네트워킹의 효율성은 떨어질 것이라는 우려가 존재했다. 그러나 브로드컴이 자체적으로 2.4만 개의 GPU를 Infiniband, 이더넷 네트워킹 방식으로 각각 테스트해 본 결과 오히려 이더넷 네트워킹의 효율성이 더 높은 것으로 나타났다 .<br><br>

        아래 차트는 Infiniband 와 이더넷의 데이터 크기별로 최대 데이터 전송 속도를 비교한 것이다 . 모든 데이터 크기에서 브로드컴의 이더넷이 엔비디아의 Infiniband 대비 최대 데이터 전송 속도가 10% 가량 더 빠르다 . 속도가 빠를수록 동일한 시간에 더 많은 데이터를 이동할 수 있다는 의미기에 작업 속도를 더 단축할 수 있다. 이더넷의 가격이 Infiniband 대비 절반 수준인 점을 고려하면 TCO 측면에서 우위에 있다고 볼 수 있다.
    </p>

    <h3>광 모듈 고장률이 높음 & 인피니밴드는 중앙 집중형 네트워크로 오류 수정에 시간 소요</h3>

    <p>
        한편, Infiniband 가 주로 사용하는 광 모듈 방식은 고장률이 높다는 단점이 있다. 최고 수준의 제품을 사용하더라도 연간 고장률이 2%~5% 수준이다 . 네트워크 모듈의 고장은 데이터센터의 효율성을 크게 낮추는 요인이다 . 예를 들어, GPU 4 천 개로 클러스터를 구축한 경우 광모듈은 8~9천 개가 필요하다 . 광모듈 9,200 개가 사용

<div style="margin-top: 40px; padding: 20px; border: 1px solid #ccc; border-radius: 8px; background-color: #f9f9f9; box-shadow: 0 2px 8px rgba(0,0,0,0.05); font-size: 13px; color: #666;">
    <h2 style="color: #333; font-size: 18px; margin-bottom: 12px;">Compliance Notice</h2>
    <p style="margin-bottom: 10px;">
        금융투자업규정 4-20조 1항5호사목에 따라 작성일 현재 사전고지와 관련한 사항이 없으며, 당사의 금융투자분석사는 자료작성일 현재 본 자료에
        관련하여 재산적 이해관계가 없습니다. 당사는 동 자료에 언급된 종목과 계열회사의 관계가 없으며 당사의 금융투자분석사는 본 자료의 작성과 관련하여
        외부 부당한 압력이나 간섭을 받지 않고 본인의 의견을 정확하게 반영하였습니다.
        본 자료는 투자자들의 투자판단에 참고가 되는 정보제공을 목적으로 배포되는 자료입니다. 본 자료에 수록된 내용은 당사 Research Center의 추정치로서
        오차가 발생할 수 있으며 정확성이나 완벽성은 보장하지 않습니다. 본 자료를 이용하시는 분은 동 자료와 관련한 투자의 최종 결정은 자신의 판단으로
        하시기 바랍니다.
    </p>
    <p style="font-size: 12px; color: #999; margin-top: 10px;">
        ※ 본 콘텐츠는 당사 리서치센터 보고서를 기반으로 생성된 요약·해설 자료입니다.
        본 자료는 인공지능(AI)을 활용하여 자동 생성되었으며, 투자자 이해를 돕기 위한 목적입니다.
        일부 내용은 원문과 차이가 있을 수 있으며, 최종 투자 판단은 원문 보고서를 참고하시기 바랍니다.
        본 자료는 투자 권유를 목적으로 하지 않으며, AI 생성 특성상 오류가 포함될 수 있습니다.
    </p>
</div>